{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08708acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b99e2f",
   "metadata": {},
   "source": [
    "# Rapid intro to unsupervised learning with Restricted Boltzmann Machines\n",
    "\n",
    "This notebook gives a rapid introduction to unsupervised learning with Restricted Boltzmann Machines (RBMs). We will again use the MNIST handwritten digits as example data, but now to goal is to teach the computer to write digits which look like those in the training data set.\n",
    "\n",
    "In contrast to supervised learning tasks, there are no labels associated with the data in unsupervised learning. Instead of predicting labels, the goal of unsupervised learning is to model the distribution $p_X(X)$ of data $X$, for example in order to identify characteristic features of the distribution.\n",
    "\n",
    "In the following I will refer to the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823) for the technical details of training a RBM.\n",
    "\n",
    "## Restricted Boltzmann Machine\n",
    "\n",
    "The RBM is an energy-based generative model defined by an energy function\n",
    "\n",
    "$$\n",
    "    E_{\\theta}(\\vec v, \\vec h) = -\\sum_{i=1}^{N_v}a_iv_i -\\sum_{\\mu=1}^{N_h}b_\\mu h_\\mu \n",
    "    -\\sum_{i=1}^{N_v}\\sum_{\\mu=1}^{N_h}W_{i\\mu}v_ih_\\mu\n",
    "$$\n",
    "\n",
    "of $N_v$ *visible units* $\\vec v$ and $N_h$ *hidden units* $\\vec h$, which take binary values $v_i,h_\\mu\\in\\{0,1\\}$. The bias vectors $\\vec a$ and $\\vec b$ together with the weight matrix $W$ make up the variational parameters $\\theta=(\\vec a, \\vec b, W)$.\n",
    "\n",
    "The corresponding joint distribution of visible and hidden units is defined as $p_{\\theta}(\\vec v, \\vec h)=e^{-E_{\\theta}(\\vec v, \\vec h)}$ (this is the \"Boltzmann\" in the name RBM). With this setup the idea is that the visible units $\\vec v$ correspond to the (high-dimensional) data and the hidden units $\\vec h$ are auxiliary degrees of freedom, which mediate correlations between different components of $\\vec v$. Hence, the goal of modeling a distribution of data means that we want to find the marginal distribution\n",
    "\n",
    "$$p_\\theta(\\vec v) = \\sum_{\\vec b\\in\\{0,1\\}^{N_h}}p_{\\theta}(\\vec v, \\vec h)$$\n",
    "\n",
    "that matches the given training data best.\n",
    "\n",
    "A suited cost function that we can aim to **maximize** for this purpose is the **log-likelihood**\n",
    "\n",
    "$$\\mathcal L(\\theta)=-\\frac{1}{|\\mathcal T_X|}\\sum_{\\vec x\\in\\mathcal T_X}\\log\\big(p_\\theta(\\vec x)\\big)$$\n",
    "\n",
    "where $\\mathcal T_X$ denotes the training data set.\n",
    "\n",
    "For details see chapter XV in [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798044cc",
   "metadata": {},
   "source": [
    "## Training data: MNIST\n",
    "\n",
    "Like in the supervised learning example, we start by loading the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataAll = tfds.as_numpy(tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False))['image'].reshape(-1,28,28)\n",
    "trainLabels = tfds.as_numpy(tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False))['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c3d35",
   "metadata": {},
   "source": [
    "Training the RBM directly on the ten different 8-bit grayscale images will be too ambitious for our purposes. Therefore, we transform the images from grayscale to binary black-and-white images and we group the images by their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dde31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataAll = trainDataAll // 128 # Transform grayscale to black-and-white\n",
    "\n",
    "# Create a dictionary, where each item corresponds to examples of one type of digit\n",
    "trainData = {}\n",
    "for n in range(10):\n",
    "    trainData[str(n)] = trainDataAll[np.where(trainLabels==n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530789fd",
   "metadata": {},
   "source": [
    "The `plot_images` function below plot `rows`x`cols` randomly selected examples from a \"stack\" of images, i.e., a 3-dimensional array, where the last two dimensions correspond to the image dimensions.\n",
    "\n",
    "Let's look at some example digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(data, rows=4, cols=4, figsize=(7,7)):\n",
    "    # For a stack of images `data` (3d-array), plot `rows`x`cols` randomly selected examples\n",
    "    \n",
    "    fig, axs = plt.subplots(rows,cols,figsize=figsize)\n",
    "\n",
    "    for ax_row in axs: \n",
    "        for ax in ax_row:\n",
    "            idx = random.randint(0,len(data)) \n",
    "            ax.imshow(data[idx])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    fig.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "# Plot some example digits\n",
    "plot_images(trainData[\"8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b515d3e3",
   "metadata": {},
   "source": [
    "## Gibbs sampling\n",
    "\n",
    "The RBM distribution $p_{\\theta}(\\vec v, \\vec h)$ has the useful property that the conditional distributions of hidden or visible units factorize as\n",
    "\n",
    "$$\n",
    "p(\\vec v|\\vec h)=\\prod_ip(v_i|\\vec h)\\\\\n",
    "p(\\vec h|\\vec v)=\\prod_\\mu p(h_\\mu|\\vec v)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "p(v_i=1|\\vec h)=\\sigma(a_i+\\sum_\\mu W_{i\\mu}h_\\mu)\\\\\n",
    "p(h_\\mu=1|\\vec v)=\\sigma(b_\\mu+\\sum_i W_{i\\mu}v_i)\n",
    "$$\n",
    "\n",
    "This enables a Markov Chain Monte Carlo scheme called **Gibbs** sampling, where realizations of $\\vec v$ and $\\vec h$ are sampled *directly* using the conditional distributions above, see Fig. 62 in [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823).\n",
    "\n",
    "**Let's implement this:**\n",
    "\n",
    "*Hint:* Given the a `jax.random.PRNGKey` and a vector of Bernoulli probabilites `jax.random.bernoulli` ([documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.bernoulli.html#jax.random.bernoulli)) generates a sample of corresponding outcomes. The returned data type is `bool` - we should cast it to an `int` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_h_given_v(v, W, b):\n",
    "    # Compute the vector p(h_mu | v)\n",
    "    \n",
    "    # TO IMPLEMENT #\n",
    "\n",
    "\n",
    "def p_v_given_h(h, W, a):\n",
    "    # Compute the vector p(v_i | h)\n",
    "    \n",
    "    # TO IMPLEMENT #\n",
    "\n",
    "\n",
    "def gibbs_step(v, W, a, b, key):\n",
    "    # This function performs one step of Gibbs sampling by sampling\n",
    "    # a new hidden outcome followed by a new outcome of visible units\n",
    "    #\n",
    "    # Input arguments:  v - starting configuration of visible units\n",
    "    #                   key - jax.random.PRNGKey\n",
    "\n",
    "    # sample a realization from p(h_mu | v)\n",
    "    \n",
    "    # TO IMPLEMENT #\n",
    "    \n",
    "    # sample a realization from p(v_i | h)\n",
    "    \n",
    "    # TO IMPLEMENT #\n",
    "    \n",
    "    return v_new\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(5,))               # JIT for performance; `n` has to be static because of loop\n",
    "@partial(jax.vmap,in_axes=(0,None,None,None,0,None)) # Vectorization of the first dimension of `v` and `key`\n",
    "def gibbs_sample(v, W, a, b, key, n):\n",
    "    # Starting from a visible configuration `v` this function performs\n",
    "    # `n` steps of Gibbs sampling and returns the new configuration\n",
    "    sample = v\n",
    "    \n",
    "    for j in range(n):\n",
    "        k1, key = jax.random.split(key)\n",
    "        sample = gibbs_step(sample, W, a, b, k1)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221bf13",
   "metadata": {},
   "source": [
    "## Gradients with Contrastive Divergence\n",
    "\n",
    "Due to the particular form of the RBM, the gradients of our cost function (log-likelihood) have a simple form:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial W_{i\\mu}}\n",
    "=\n",
    "\\langle v_ih_\\mu\\rangle_{\\text{data}}-\\langle v_ih_\\mu\\rangle_{\\text{model}}\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial a_i}\n",
    "=\n",
    "\\langle v_i\\rangle_{\\text{data}}-\\langle v_i\\rangle_{\\text{model}}\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial b_\\mu}\n",
    "=\n",
    "\\langle h_\\mu\\rangle_{\\text{data}}-\\langle h_\\mu\\rangle_{\\text{model}}\n",
    "$$\n",
    "\n",
    "Here, $\\langle \\cdot\\rangle_{\\text{data}}$ denotes a mean over the training data and $\\langle \\cdot\\rangle_{\\text{model}}$ denotes the mean over a sample drawn from our RBM distribution $p_{\\theta}(\\vec v)$. Since $p(\\vec v, \\vec h)=p(\\vec v)p(\\vec h|\\vec v)$ and $h_\\mu\\in\\{0,1\\}$, the empirical means can be rewritten, e.g. as\n",
    "\n",
    "$$\n",
    "\\langle v_ih_\\mu\\rangle_{\\mathcal S}=\\frac{1}{|\\mathcal S|}\\sum_{\\vec v\\in\\mathcal S} v_ip_\\theta(h_\\mu|\\vec v)\n",
    "$$\n",
    "\n",
    "where $\\mathcal S$ denotes the training data set or a sample drawn from $p_{\\theta}(\\vec v)$.\n",
    "\n",
    "**Let's implement this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap,in_axes=(0, None, None))\n",
    "def wgrad_helper(v, W, b):\n",
    "    return jnp.outer(v, p_h_given_v(v,W,b))\n",
    "\n",
    "\n",
    "@partial(jax.vmap,in_axes=(0, None, None))\n",
    "def bgrad_helper(v, W, b):\n",
    "    return p_h_given_v(v,W,b)\n",
    "    \n",
    "\n",
    "def pcd_gradients(batch, W, a, b, key, n=1, modelSample=None):\n",
    "    \n",
    "    batchSize = batch.shape[0]\n",
    "    \n",
    "    keys = jax.random.split(key, batch.shape[0])\n",
    "    \n",
    "    if modelSample is None:\n",
    "        modelSample = batch\n",
    "    \n",
    "    # Sample outcomes from the RBM\n",
    "    modelSample = gibbs_sample(modelSample.reshape((batchSize, -1)), W, a, b, keys, n)\n",
    "    \n",
    "    # Flatten the input batch\n",
    "    flatBatch = batch.reshape((batchSize, -1))\n",
    "    \n",
    "    W_grad = # TO IMPLEMENT #\n",
    "    \n",
    "    a_grad = # TO IMPLEMENT #\n",
    "    \n",
    "    b_grad = # TO IMPLEMENT #\n",
    "    \n",
    "    return W_grad, a_grad, b_grad, modelSample.reshape(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911ae09",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The function below implements the training loop.\n",
    "\n",
    "Input parameters are\n",
    "\n",
    "- `W`: weight matrix\n",
    "- `a`: visible bias\n",
    "- `b`: hidden bias\n",
    "- `trainData`: Training data. 3-dimensional array, where the last two dimensions are image dimensions.\n",
    "- `learningRate`: learning rate\n",
    "- `numEpochs`: number of epochs for training\n",
    "- `batchSize`: batchSize\n",
    "- `cg_n`: number of iterations between samples in the Gibbs MCMC sampling\n",
    "- `persistent`: boolean indicating whether to perform persistent contrastive divergence or not\n",
    "- `seed`: seed for random number generator\n",
    "\n",
    "The function returns the RBM parameters obtained at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738df590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W, a, b, trainData, learningRate=0.01, numEpochs=10, batchSize=128, cg_n=2, persistent=False, seed=1234):\n",
    "\n",
    "    prng_key = jax.random.PRNGKey(seed)\n",
    "    modelSample = None\n",
    "    \n",
    "    batchNumber = trainData.shape[0] // batchSize\n",
    "    \n",
    "    # Training loop over epochs\n",
    "    for n in range(numEpochs):\n",
    "\n",
    "        print(\"Epoch %d\" % (n))\n",
    "\n",
    "        # Generate randomly shuffled batches\n",
    "        prng_key, tmp_key = jax.random.split(prng_key) # jax-style treatment of random numbers\n",
    "        batches = jax.random.permutation(tmp_key, trainData)[:batchNumber*batchSize]\n",
    "        batches = batches.reshape(-1,batchSize,28,28)\n",
    "\n",
    "        for batch in batches:\n",
    "\n",
    "            prng_key, tmp_key = jax.random.split(prng_key)\n",
    "\n",
    "            if not persistent:\n",
    "                modelSample = None\n",
    "            \n",
    "            # Compute gradients\n",
    "            Wg, ag, bg, modelSample = pcd_gradients(batch, W, a, b, tmp_key, n=cg_n, modelSample=modelSample)\n",
    "\n",
    "            # Update parameters with gradients\n",
    "            \n",
    "            # TO IMPLEMENT #\n",
    "\n",
    "        plot_images(modelSample) # Show some example images generated by the RBM\n",
    "        \n",
    "    return W, a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554033a",
   "metadata": {},
   "source": [
    "Finally, we are set to train the RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVisible = 28*28\n",
    "numHidden = 256 # number of hidden units\n",
    "\n",
    "# Random initialization of parameters:\n",
    "W = 0.01 * jax.random.normal(jax.random.PRNGKey(1234), (numVisible, numHidden))\n",
    "a = jnp.zeros((numVisible,))\n",
    "b = jnp.zeros((numHidden,))\n",
    "\n",
    "# Run training\n",
    "W, a, b = train(W, a, b, trainData[\"8\"], numEpochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19256eae",
   "metadata": {},
   "source": [
    "## Inspecting the features\n",
    "\n",
    "Now we can inspect which features were learned in the weight matrix $W$ by plotting individual lines reshaped to the image dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c87b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(jnp.transpose(W).reshape(-1,28,28), rows=10, cols=10, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc08588",
   "metadata": {},
   "source": [
    "## Learning multiple digits\n",
    "\n",
    "The RBM can not only learn to generate one single digit. Let's learn two at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVisible = 28*28\n",
    "numHidden = 256 # number of hidden units\n",
    "\n",
    "# Random initialization of parameters:\n",
    "W = 0.01 * jax.random.normal(jax.random.PRNGKey(1234), (numVisible, numHidden))\n",
    "a = jnp.zeros((numVisible,))\n",
    "b = jnp.zeros((numHidden,))\n",
    "\n",
    "examples = jnp.concatenate([trainData[\"8\"], trainData[\"4\"]])\n",
    "\n",
    "# Run training\n",
    "W, a, b = train(W, a, b, examples, numEpochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(jnp.transpose(W).reshape(-1,28,28), rows=10, cols=10, figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_seminar]",
   "language": "python",
   "name": "conda-env-ml_seminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
