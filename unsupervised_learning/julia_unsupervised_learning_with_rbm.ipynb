{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using PyPlot\n",
    "using Random, Statistics\n",
    "using Flux: onehotbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-infrared",
   "metadata": {},
   "source": [
    "# Rapid intro to unsupervised learning with Restricted Boltzmann Machines\n",
    "\n",
    "This notebook gives a rapid introduction to unsupervised learning with Restricted Boltzmann Machines (RBMs). We will again use the MNIST handwritten digits as example data, but now to goal is to teach the computer to write digits which look like those in the training data set. This is a very basic example of an application of generative modelling; click [here](https://thispersondoesnotexist.com/) to see some impressive results generated using a more sophisticated approach in the same spirit.\n",
    "\n",
    "In contrast to supervised learning tasks, there are no labels associated with the data in unsupervised learning. Instead of predicting labels, the goal of unsupervised learning is to model the distribution $p_X(X)$ of data $X$, for example in order to identify characteristic features of the distribution.\n",
    "\n",
    "In the following I will refer to the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823) for the technical details of training a RBM.\n",
    "\n",
    "## Restricted Boltzmann Machine\n",
    "\n",
    "The RBM is an energy-based generative model defined by an energy function\n",
    "\n",
    "$$\n",
    "    E_{\\theta}(\\vec v, \\vec h) = -\\sum_{i=1}^{N_v}a_iv_i -\\sum_{\\mu=1}^{N_h}b_\\mu h_\\mu \n",
    "    -\\sum_{i=1}^{N_v}\\sum_{\\mu=1}^{N_h}W_{i\\mu}v_ih_\\mu\n",
    "$$\n",
    "\n",
    "of $N_v$ *visible units* $\\vec v$ and $N_h$ *hidden units* $\\vec h$, which take binary values $v_i,h_\\mu\\in\\{0,1\\}$. The bias vectors $\\vec a$ and $\\vec b$ together with the weight matrix $W$ make up the variational parameters $\\theta=(\\vec a, \\vec b, W)$.\n",
    "\n",
    "The corresponding joint distribution of visible and hidden units is defined as $p_{\\theta}(\\vec v, \\vec h)=e^{-E_{\\theta}(\\vec v, \\vec h)}$ (this is the \"Boltzmann\" in the name RBM). With this setup the idea is that the visible units $\\vec v$ correspond to the (high-dimensional) data and the hidden units $\\vec h$ are auxiliary degrees of freedom, which mediate correlations between different components of $\\vec v$. Hence, the goal of modeling a distribution of data means that we want to find the marginal distribution\n",
    "\n",
    "$$p_\\theta(\\vec v) = \\sum_{\\vec b\\in\\{0,1\\}^{N_h}}p_{\\theta}(\\vec v, \\vec h)$$\n",
    "\n",
    "that matches the given training data best.\n",
    "\n",
    "A suited cost function that we can aim to **maximize** for this purpose is the **log-likelihood**\n",
    "\n",
    "$$\\mathcal L(\\theta)=-\\frac{1}{|\\mathcal T_X|}\\sum_{\\vec x\\in\\mathcal T_X}\\log\\big(p_\\theta(\\vec x)\\big)$$\n",
    "\n",
    "where $\\mathcal T_X$ denotes the training data set.\n",
    "\n",
    "For details see chapter XV in [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-scientist",
   "metadata": {},
   "source": [
    "## Training data: MNIST\n",
    "\n",
    "Like in the supervised learning example, we start by loading the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full training set\n",
    "trainDataAll, trainLabels = MNIST.traindata()\n",
    "trainDataAll = convert.(Int32, floor.(255*trainDataAll));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-telephone",
   "metadata": {},
   "source": [
    "Training the RBM directly on the ten different 8-bit grayscale images will be too ambitious for our purposes. Therefore, we transform the images from grayscale to binary black-and-white images and we group the images by their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataAll = div.(trainDataAll, 128)\n",
    "\n",
    "trainData = Dict()\n",
    "\n",
    "for n in 0:9\n",
    "    trainData[string(n)] = trainDataAll[:,:,findall(trainLabels.==n)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-mileage",
   "metadata": {},
   "source": [
    "The `plot_images` function below plot `rows`x`cols` randomly selected examples from a \"stack\" of images, i.e., a 3-dimensional array, where the last two dimensions correspond to the image dimensions.\n",
    "\n",
    "Let's look at some example digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_images(data; rows=4, cols=4, figsize=(7,7))\n",
    "    # For a stack of images `data` (3d-array), plot `rows`x`cols` randomly selected examples\n",
    "    \n",
    "    fig, axs = subplots(rows,cols,figsize=figsize)\n",
    "    \n",
    "    for i in 1:rows*cols\n",
    "        idx = rand(1:size(data)[3])\n",
    "        axs[i].imshow(transpose(data[:,:,idx]))\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "    end\n",
    "    tight_layout() \n",
    "    show()\n",
    "end\n",
    "            \n",
    "# Plot some example digits\n",
    "plot_images(trainData[\"8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-handle",
   "metadata": {},
   "source": [
    "## Gibbs sampling\n",
    "\n",
    "The RBM distribution $p_{\\theta}(\\vec v, \\vec h)$ has the useful property that the conditional distributions of hidden or visible units factorize as\n",
    "\n",
    "$$\n",
    "p(\\vec v|\\vec h)=\\prod_ip(v_i|\\vec h)\\\\\n",
    "p(\\vec h|\\vec v)=\\prod_\\mu p(h_\\mu|\\vec v)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "p(v_i=1|\\vec h)=\\sigma(a_i+\\sum_\\mu W_{i\\mu}h_\\mu)\\\\\n",
    "p(h_\\mu=1|\\vec v)=\\sigma(b_\\mu+\\sum_i W_{i\\mu}v_i)\n",
    "$$\n",
    "\n",
    "This enables a Markov Chain Monte Carlo scheme called **Gibbs** sampling, where realizations of $\\vec v$ and $\\vec h$ are sampled *directly* using the conditional distributions above, see Fig. 62 in [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823).\n",
    "\n",
    "**Let's implement this:**\n",
    "\n",
    "*Hint:* We can draw random bernoulli outcomes according to a vector of probabilities `p` with `[rand() < q for q in p]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid(x)\n",
    "    return 1. / (1. + exp.(-x))\n",
    "end\n",
    "\n",
    "function p_h_given_v(v, W, b)\n",
    "    # Compute the vector p(h_mu | v)\n",
    "    \n",
    "    return sigmoid.(b + transpose(W) * v)\n",
    "end\n",
    "\n",
    "\n",
    "function p_v_given_h(h, W, a)\n",
    "    # Compute the vector p(v_i | h)\n",
    "    \n",
    "    return sigmoid.(a + W * h)\n",
    "end\n",
    "\n",
    "\n",
    "function gibbs_step(v, W, a, b)\n",
    "    # This function performs one step of Gibbs sampling by sampling\n",
    "    # a new hidden outcome followed by a new outcome of visible units\n",
    "    #\n",
    "    # Input arguments:  v - starting configuration of visible units\n",
    "    #                   key - jax.random.PRNGKey\n",
    "\n",
    "    # sample a realization from p(h_mu | v)\n",
    "    \n",
    "    p = p_h_given_v(v, W, b)\n",
    "    h = [rand() < q for q in p]\n",
    "    \n",
    "    # sample a realization from p(v_i | h)\n",
    "    \n",
    "    p = p_v_given_h(h, W, a)\n",
    "    v_new = [rand() < q for q in p]\n",
    "    \n",
    "    return v_new\n",
    "end\n",
    "\n",
    "function gibbs_sample(v, W, a, b, n)\n",
    "    # Starting from a visible configuration `v` this function performs\n",
    "    # `n` steps of Gibbs sampling and returns the new configuration\n",
    "    sample = v\n",
    "    \n",
    "    for j in 1:n\n",
    "        for k in 1:size(v)[2]\n",
    "            sample[:,k] = gibbs_step(sample[:,k], W, a, b)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return sample\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-elements",
   "metadata": {},
   "source": [
    "## Gradients with Contrastive Divergence\n",
    "\n",
    "Due to the particular form of the RBM, the gradients of our cost function (log-likelihood) have a simple form:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial W_{i\\mu}}\n",
    "=\n",
    "\\langle v_ih_\\mu\\rangle_{\\text{data}}-\\langle v_ih_\\mu\\rangle_{\\text{model}}\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial a_i}\n",
    "=\n",
    "\\langle v_i\\rangle_{\\text{data}}-\\langle v_i\\rangle_{\\text{model}}\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal L(W,\\vec a, \\vec b)}{\\partial b_\\mu}\n",
    "=\n",
    "\\langle h_\\mu\\rangle_{\\text{data}}-\\langle h_\\mu\\rangle_{\\text{model}}\n",
    "$$\n",
    "\n",
    "Here, $\\langle \\cdot\\rangle_{\\text{data}}$ denotes a mean over the training data and $\\langle \\cdot\\rangle_{\\text{model}}$ denotes the mean over a sample drawn from our RBM distribution $p_{\\theta}(\\vec v)$. Since $p(\\vec v, \\vec h)=p(\\vec v)p(\\vec h|\\vec v)=p(\\vec v)\\prod_{\\mu}p(h_\\mu|\\vec v)$ and $h_\\mu\\in\\{0,1\\}$, \n",
    "\n",
    "$$\n",
    "\\langle v_ih_\\mu\\rangle\n",
    "=\\sum_{\\vec v,\\vec h}p(\\vec v, \\vec h) v_ih_\\mu\n",
    "=\\sum_{\\vec v}p(\\vec v)\\sum_{\\vec h}p(\\vec h|\\vec v) v_ih_\\mu\n",
    "=\\sum_{\\vec v}p(\\vec v)\\sum_{h_\\mu\\in\\{0,1\\}}p(h_\\mu|\\vec v) v_ih_\\mu\n",
    "=\\sum_{\\vec v}p(\\vec v)p(h_\\mu=1|\\vec v) v_i\n",
    "$$\n",
    "\n",
    "and the empirical means can be rewritten, e.g. as\n",
    "\n",
    "$$\n",
    "\\langle v_ih_\\mu\\rangle_{\\mathcal S}\n",
    "=\\frac{1}{|\\mathcal S|}\\sum_{\\vec v\\in\\mathcal S} v_ip_\\theta(h_\\mu=1|\\vec v)\n",
    "$$\n",
    "\n",
    "where $\\mathcal S$ denotes the training data set or a sample drawn from $p_{\\theta}(\\vec v)$.\n",
    "\n",
    "**Let's implement this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "function pcd_gradients(batch, W, a, b; n=1, modelSample=nothing)\n",
    "    \n",
    "    batchSize = size(batch)[3]\n",
    "    imgSize = size(batch)[1] * size(batch)[2]\n",
    "    \n",
    "    if modelSample == nothing\n",
    "        modelSample = copy(batch)\n",
    "    end\n",
    "    \n",
    "    # Sample outcomes from the RBM\n",
    "    modelSample = gibbs_sample(reshape(modelSample, (imgSize, batchSize)), W, a, b, n)\n",
    "    \n",
    "    # Flatten the input batch\n",
    "    flatBatch = reshape(batch, (imgSize, batchSize))\n",
    "    \n",
    "    # Compute W-gradients\n",
    "    W_grad = zeros(size(W))\n",
    "    for j in 1:batchSize\n",
    "        W_grad .+= flatBatch[:,j] .* transpose(p_h_given_v(flatBatch[:,j], W, b))\n",
    "        W_grad .-= modelSample[:,j] .* transpose(p_h_given_v(modelSample[:,j], W, b))\n",
    "    end\n",
    "    W_grad ./= batchSize\n",
    "    \n",
    "    # Compute a-gradients\n",
    "    a_grad = mean(flatBatch, dims=2) - mean(modelSample, dims=2)\n",
    "    a_grad = reshape(a_grad, :)\n",
    "    \n",
    "    # Compute b-gradients\n",
    "    b_grad = zeros(size(b))\n",
    "    for j in 1:batchSize\n",
    "        b_grad .+= p_h_given_v(flatBatch[:,j], W, b) - p_h_given_v(modelSample[:,j], W, b)\n",
    "    end\n",
    "    b_grad ./= batchSize\n",
    "    \n",
    "    return W_grad, a_grad, b_grad, reshape(modelSample, size(batch))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-killer",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The function below implements the training loop.\n",
    "\n",
    "Input parameters are\n",
    "\n",
    "- `W`: weight matrix\n",
    "- `a`: visible bias\n",
    "- `b`: hidden bias\n",
    "- `trainData`: Training data. 3-dimensional array, where the last two dimensions are image dimensions.\n",
    "- `learningRate`: learning rate\n",
    "- `numEpochs`: number of epochs for training\n",
    "- `batchSize`: batchSize\n",
    "- `cg_n`: number of iterations between samples in the Gibbs MCMC sampling\n",
    "- `persistent`: boolean indicating whether to perform persistent contrastive divergence or not\n",
    "- `seed`: seed for random number generator\n",
    "\n",
    "The function returns the RBM parameters obtained at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(W, a, b, trainData; learningRate=0.01, numEpochs=10, batchSize=128, cg_n=2, persistent=false, seed=1234)\n",
    "\n",
    "    Random.seed!(seed)\n",
    "    \n",
    "    modelSample = nothing\n",
    "    \n",
    "    batchNumber = div(size(trainData)[3], batchSize)\n",
    "    \n",
    "    # Training loop over epochs\n",
    "    for n in 1:numEpochs\n",
    "\n",
    "        println(\"Epoch $n\")\n",
    "\n",
    "        # Generate randomly shuffled batches\n",
    "        order = shuffle(1:size(trainData)[3])\n",
    "        batches = reshape(trainData[:,:,order][:,:,1:Int(batchNumber*batchSize)], 28,28,batchSize,:)\n",
    "\n",
    "        for k in 1:batchNumber\n",
    "            batch = batches[:,:,:,k]\n",
    "\n",
    "            if !persistent\n",
    "                modelSample = nothing\n",
    "            end\n",
    "            \n",
    "            # Compute gradients\n",
    "            Wg, ag, bg, modelSample = pcd_gradients(batch, W, a, b, n=cg_n, modelSample=modelSample)\n",
    "\n",
    "            # Update parameters with gradients\n",
    "            \n",
    "            W .+= learningRate .* Wg\n",
    "            a .+= learningRate .* ag\n",
    "            b .+= learningRate .* bg\n",
    "        end\n",
    "        \n",
    "        plot_images(modelSample) # Show some example images generated by the RBM\n",
    "    end\n",
    "    return W, a, b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-proposal",
   "metadata": {},
   "source": [
    "Finally, we are set to train the RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVisible=28*28\n",
    "numHidden = 256\n",
    "\n",
    "W = 0.01 * randn((numVisible, numHidden))\n",
    "a = zeros(numVisible)\n",
    "b = zeros(numHidden)\n",
    "\n",
    "W, a, b = train(W, a, b, trainData[\"8\"], numEpochs=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-festival",
   "metadata": {},
   "source": [
    "## Inspecting the features\n",
    "\n",
    "Now we can inspect which features were learned in the weight matrix $W$ by plotting individual lines reshaped to the image dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(reshape(W,(28,28,:)),rows=10,cols=10,figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-purple",
   "metadata": {},
   "source": [
    "## Learning multiple digits\n",
    "\n",
    "The RBM can not only learn to generate one single digit. Let's learn two at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "numVisible=28*28\n",
    "numHidden = 256\n",
    "\n",
    "W = 0.01 * randn((numVisible, numHidden))\n",
    "a = zeros(numVisible)\n",
    "b = zeros(numHidden)\n",
    "\n",
    "examples = cat(trainData[\"8\"],trainData[\"4\"], dims=3)\n",
    "\n",
    "W, a, b = train(W, a, b, examples, numEpochs=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(reshape(W,(28,28,:)),rows=10,cols=10,figsize=(10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
