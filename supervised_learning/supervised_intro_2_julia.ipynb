{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using PyPlot\n",
    "using Random, Statistics\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid intro to supervised learning with neural nets I: from scratch\n",
    "\n",
    "This notebook gives a rapid introduction to supervised learning with neural networks. The example is based on [Chapter 1 of Nielsen's online book \"Neural Networks and Deep Learning\"](http://neuralnetworksanddeeplearning.com/chap1.html) and it guides you to set up the neural network training using the [Flux](https://fluxml.ai/Flux.jl/stable/) Julia package.\n",
    "\n",
    "For further reading I recommend also the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few words about Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST hand-written digits data set\n",
    "\n",
    "Let's first get a simple exemplary data set - the MNIST hand-written digits. The following cell downloads both the test and training parts of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full training set\n",
    "train_x, train_y = float.(MNIST.traindata())\n",
    "# load full test set\n",
    "test_x,  test_y  = float.(MNIST.testdata());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainData` is now a array of shape `(28,28, 60000)`, meaning that we have 60k images of 28$\\times$28 pixels (grayscale), each showing one hand-written digit. `trainLabels` holds the corresponding *labels*, i.e. an integer for each image, stating which digit it shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a neural network model using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_model (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function build_model(layers; imgsize=(28,28))\n",
    "    m = Dense(prod(imgsize), layers[1], sigmoid)\n",
    "    for j in 2:length(layers)\n",
    "        m = Chain(m, Dense(layers[j-1], layers[j],sigmoid))\n",
    "    end\n",
    "    return m\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again what the network thinks about our images of digits. For this purpose we define `initialize_network` and `neural_network` analogous to part I of the tutorial, but this time based on our `MyNet` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 10-element Vector{Float32}:\n",
       " 0.428681f0\n",
       " 0.64658606f0\n",
       " 0.52417076f0\n",
       " 0.61604553f0\n",
       " 0.4864485f0\n",
       " 0.5423035f0\n",
       " 0.25397527f0\n",
       " 0.4249637f0\n",
       " 0.73968923f0\n",
       " 0.3823749f0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_layers=[100,10]\n",
    "\n",
    "neural_network = build_model(net_layers)\n",
    "params = Flux.params(neural_network) \n",
    "\n",
    "neural_network(reshape(train_x[:,:,1],28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a cost function. This is the same as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost_function (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cost_function(predictions, labels)\n",
    "    \"\"\"This function evaluates the cost function for given predictions and labels\n",
    "    Args:\n",
    "    * predictions: Predictions from neural net. Array of shape mathcal T x 10.\n",
    "    * labels: Correct labels for the corresponding images. Array of mathcal T integers.\n",
    "    Returns: Cost associated with the neural network predictions for the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = Flux.onehotbatch(labels, 0:9)\n",
    "\n",
    "    cost = sum((predictions-labels).^2)\n",
    "    return cost / size(labels)[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can check the performance of our randomly initialized network in classifying some of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7701077f0 (tracked)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_x[:,:,1:128]    # select a batch of images\n",
    "labels = train_y[1:128] # and corresponding labels\n",
    "\n",
    "# ! compute neural network predictions\n",
    "predictions = neural_network(reshape(batch, 28*28,size(batch)[3]))\n",
    "\n",
    "# ! evaluate the cost function\n",
    "cost_function(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is missing is a function to compute the gradients of the cost function. This is easily solved using `Flux.gradient()` for automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost_function_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cost_function_gradient(net, params, batch, labels)\n",
    "    return Flux.gradient(() -> cost_function(net(reshape(batch, 28*28,size(batch)[3])),labels), params) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost: 2.9444165f0 (tracked)\n",
      "Correctly predicted labels: 1067 / 10000\n",
      "Episode 1\n",
      "Current cost: 1.0339497f0 (tracked)\n",
      "Correctly predicted labels: 2503/10000\n",
      "Episode 2\n",
      "Current cost: 0.9229258f0 (tracked)\n",
      "Correctly predicted labels: 3013/10000\n",
      "Episode 3\n",
      "Current cost: 0.89791876f0 (tracked)\n",
      "Correctly predicted labels: 3280/10000\n",
      "Episode 4\n",
      "Current cost: 0.8880436f0 (tracked)\n",
      "Correctly predicted labels: 3401/10000\n",
      "Episode 5\n",
      "Current cost: 0.8825424f0 (tracked)\n",
      "Correctly predicted labels: 3452/10000\n",
      "Episode 6\n",
      "Current cost: 0.8786173f0 (tracked)\n",
      "Correctly predicted labels: 3505/10000\n",
      "Episode 7\n",
      "Current cost: 0.87532216f0 (tracked)\n",
      "Correctly predicted labels: 3533/10000\n",
      "Episode 8\n",
      "Current cost: 0.8722753f0 (tracked)\n",
      "Correctly predicted labels: 3553/10000\n",
      "Episode 9\n",
      "Current cost: 0.8693074f0 (tracked)\n",
      "Correctly predicted labels: 3572/10000\n",
      "Episode 10\n",
      "Current cost: 0.8663381f0 (tracked)\n",
      "Correctly predicted labels: 3612/10000\n"
     ]
    }
   ],
   "source": [
    "function evaluate_predictions(predictions, labels)\n",
    "    \"\"\"This is a helper function that counts how many of the given predictions match the labels.\n",
    "    Args:\n",
    "    * `predictions`: Predictions from neural network (=activations on output layer)\n",
    "    * `labels`: correct labels\n",
    "    Returns: Number of correct predictions, i.e., number of cases, in which the index of the maximal \n",
    "    activation matches the given label.\n",
    "    \"\"\"\n",
    "    pred_labels = [Int(findmax(predictions[:,i])[2])-1 for i in 1:size(predictions)[2]]\n",
    "        \n",
    "    return sum(pred_labels .== labels)\n",
    "end\n",
    "\n",
    "\n",
    "prng_key = Random.seed!(1234)\n",
    "\n",
    "neural_network = build_model(net_layers)\n",
    "params = Flux.params(neural_network) \n",
    "\n",
    "# Here we define the hyperparamters\n",
    "num_epochs = 10 # Number of epochs to loop over\n",
    "learning_rate = 0.001 # Learning rate\n",
    "batch_size = 128 # Size of mini-batches\n",
    "\n",
    "# Compute the number of mini-batches that matches the chosen mini-batch size\n",
    "batch_number = floor(Int,size(train_x)[end] / batch_size)\n",
    "\n",
    "# Evaluate network and assess performance\n",
    "predictions = neural_network(reshape(test_x,28*28,size(test_x)[3]))\n",
    "current_cost = cost_function(predictions, test_y)\n",
    "correct_predictions = evaluate_predictions(predictions, test_y)\n",
    "println(\"Initial cost: $(current_cost)\")\n",
    "println(\"Correctly predicted labels: $(correct_predictions) / $(length(test_y))\")\n",
    "\n",
    "for n in 1:num_epochs\n",
    "    \n",
    "    println(\"Episode $(n)\")\n",
    "    order = shuffle(1:length(train_y))\n",
    "    samples, labels = ( reshape(train_x[:,:,order][:,:,1:Int(batch_number*batch_size)], 28,28,128,:), \n",
    "        reshape(train_y[order][1:Int(batch_number*batch_size)], 128,:))\n",
    "\n",
    "    \n",
    "    for i in 1:batch_number\n",
    "\n",
    "        # Compute gradients\n",
    "        gs=cost_function_gradient(neural_network, params, samples[:,:,:,i], labels[:,i])\n",
    "  \n",
    "        # Perform SGD parameter update step\n",
    "        for p in params\n",
    "            Flux.Optimise.update!(p,-learning_rate*gs[p])\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    # Evaluate network and assess performance\n",
    "    predictions = neural_network(reshape(test_x,28*28,size(test_x)[3]))\n",
    "    current_cost = cost_function(predictions, test_y)    \n",
    "    correct_predictions = evaluate_predictions(predictions, test_y)\n",
    "    println(\"Current cost: $(current_cost)\")\n",
    "    println(\"Correctly predicted labels: $(correct_predictions)/$(length(test_y))\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
