{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dcbfc",
   "metadata": {},
   "source": [
    "# Rapid intro to supervised learning with neural nets II: using JAX\n",
    "\n",
    "This notebook gives a rapid introduction to supervised learning with neural networks. The example is based on [Chapter 1 of Nielsen's online book \"Neural Networks and Deep Learning\"](http://neuralnetworksanddeeplearning.com/chap1.html) and it guides you to set up the neural network training using the [JAX](https://www.github.com/google/jax) and [Flax](https://github.com/google/flax) libraries.\n",
    "\n",
    "For further reading I recommend also the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798845b",
   "metadata": {},
   "source": [
    "## A few words on JAX\n",
    "\n",
    "[JAX](https://www.github.com/google/jax) is a Python library that provides useful functionality for machine learning applications (especially deep learning), namely automatic differentiation, just-in-time compilation, and vectorization. This is implemented in JAX through function transformations, i.e., functions that map functions to new functions.\n",
    "\n",
    "### Automatic differentiation\n",
    "With [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) we can let the computer compute gradients of arbitrary functions. In JAX the function `jax.grad()` takes a function as argument and returns a function that is the gradient of the given function. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7538f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Get the gradient\n",
    "f_prime = jax.grad(f)\n",
    "\n",
    "# Evaluate function and gradient\n",
    "x=np.arange(-1,1,.1)\n",
    "y=np.array([f_prime(r) for r in x])\n",
    "plt.plot(x,f(x),label=r\"$f(x)$\")\n",
    "plt.plot(x,y,label=r\"$f'(x)$\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4499c84",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "The `jax.vmap()` function allows us to apply a function that is defined for a single input to an array. For example, we can replace the line\n",
    "```python\n",
    "y=np.array([f_prime(r) for r in x])\n",
    "```\n",
    "from the previous cell as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vectorized version of the function\n",
    "f_prime_vectorized = jax.vmap(f_prime)\n",
    "# Apply the vectorized function to an input array\n",
    "f_prime_vectorized(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8093178",
   "metadata": {},
   "source": [
    "### Just-in-time compilation\n",
    "With `jax.jit()` we can ask JAX to just-in-time (JIT) compile our Python code such that it can be executed with the high efficiency of [XLA](https://www.tensorflow.org/xla).\n",
    "\n",
    "A simple, yet not particularly spectacular, example is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad101ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vectorized version of the function\n",
    "f_prime_vectorized_compiled = jax.jit(f_prime_vectorized)\n",
    "# Apply the vectorized function to an input array\n",
    "f_prime_vectorized_compiled(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dfe40",
   "metadata": {},
   "source": [
    "Superficially, `f_prime_vectorized_compiled()` does not differ from `f_prime_vectorized()`. Under the hood, however, the compiled function executes its task (potentially) much more efficiently than the original one. In the code below you will find a number of `jax.jit` statements. By removing these statements and working only with the un-compiled versions of the respective functions you will observe a noticeable slow-down.\n",
    "\n",
    "**Notice:** In order to use JAX function transformations (like vectorization) we have to replace the Numpy library with its JAX version, which has the same interface. For example, instead of `np.array` all our arrays will be `jnp.array`s. The same for functions, e.g. instead of `np.dot` we have to use `jnp.dot` for the dot-product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8ac0a",
   "metadata": {},
   "source": [
    "## The MNIST hand-written digits data set\n",
    "\n",
    "Let's first get a simple exemplary data set - the MNIST hand-written digits. The following cell downloads both the test and training parts of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = jnp.array(\n",
    "                tfds.as_numpy(\n",
    "                    tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False)\n",
    "                )['image'].reshape(-1,28,28)\n",
    "            )\n",
    "trainLabels = jnp.array(\n",
    "                    tfds.as_numpy(\n",
    "                        tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False)\n",
    "                    )['label']\n",
    "                )\n",
    "\n",
    "testData = jnp.array(\n",
    "                tfds.as_numpy(\n",
    "                    tfds.load('mnist', split='test', batch_size=-1, shuffle_files=False)\n",
    "                )['image'].reshape(-1,28,28)\n",
    "            )\n",
    "testLabels = jnp.array(\n",
    "                tfds.as_numpy(\n",
    "                    tfds.load('mnist', split='test', batch_size=-1, shuffle_files=False)\n",
    "                )['label']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64bbc",
   "metadata": {},
   "source": [
    "`trainData` is now a `jax.numpy.array` of shape `(60000,28,28)`, meaning that we have 60k images of 28$\\times$28 pixels (grayscale), each showing one hand-written digit. `trainLabels` holds the corresponding *labels*, i.e. an integer for each image, stating which digit it shows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d130d",
   "metadata": {},
   "source": [
    "## Defining a neural network model using Flax\n",
    "\n",
    "[Flax](https://github.com/google/flax) is a library built on top of JAX, which allows you to easily compose complicated deep learning models. If you are familiar with Pytorch, the following syntax will be very intuitive for you.\n",
    "\n",
    "In Flax a new model can be defined as a class that inherits from the `nn.Module` base class. Here, we introduce Flax's abbreviated model definition; [notice that general model definitions can be more involved](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html#Module-basics). In the short form, a model is defined by defining a `__call__` method that evaluates the network on the given input. The library provides implementations of [typical linear transformations](https://flax.readthedocs.io/en/latest/flax.linen.html#linear-modules) as well as [typical activation functions](https://flax.readthedocs.io/en/latest/flax.linen.html#activation-functions) (among other typical building blocks of neural networks).\n",
    "\n",
    "In the cell below we use the provided `Dense` linear transformation and the `sigmoid` activation function to implement the same network architecture as the one that we coded from scratch in part I of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    layers: Sequence[int] # A tuple that contains the widths of all layers follwing the input layer\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        a = x.ravel() # flatten the input\n",
    "\n",
    "        # Evaluate network layer by layer\n",
    "        for width in self.layers:\n",
    "            # Apply a the Dense layer with given width followed by the non-linearity\n",
    "            a = nn.sigmoid(nn.Dense(width)(a))\n",
    "            \n",
    "        # Return activations of the output layer\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc36704",
   "metadata": {},
   "source": [
    "Now we can again what the network thinks about our images of digits. For this purpose we define `initialize_network` and `neural_network` analogous to part I of the tutorial, but this time based on our `MyNet` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9eab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def initialize_network(layers, seed=123):\n",
    "    # Get random initial parameters. Notice: The `init` method needs an example input for this purpose.\n",
    "    return MyNet(layers=layers).init(jax.random.PRNGKey(seed), trainData[0])\n",
    "\n",
    "\n",
    "def neural_network(params, image, layers):\n",
    "    # Evaluate the network with given parameters\n",
    "    neural_network = MyNet(layers=layers)\n",
    "\n",
    "    return jax.jit(jax.vmap(lambda x: neural_network.apply(params, x)))(image)\n",
    "    \n",
    "# Define the network size.\n",
    "# Here we only need to include the width of layers *after* the input layer.\n",
    "# The size of the input layer is determined automatically from the input data.\n",
    "net_layers=(100,10) \n",
    "\n",
    "# Get initial parameters\n",
    "params = initialize_network(net_layers)\n",
    "\n",
    "# Evaluate the network\n",
    "neural_network(params, trainData[:3], net_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623aba70",
   "metadata": {},
   "source": [
    "Next, we need a cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=3)\n",
    "def cost_function(params, images, labels, layers):\n",
    "    '''This function evaluates the cost function for given predictions and labels\n",
    "    Args:\n",
    "    * `params`: Network parameters.\n",
    "    * `images`: A batch of input images.\n",
    "    * `labels`: Correct labels for the given images.\n",
    "    * `layers`: Size of the network (list of widths).\n",
    "    Returns: Cost associated with the neural network predictions for the given data.\n",
    "    '''\n",
    "\n",
    "    labels = jax.nn.one_hot(labels, 10) # get one-hot encoding of labels\n",
    "    predictions = neural_network(params,images,layers)\n",
    "    cost = jnp.sum((predictions-labels)**2)\n",
    "\n",
    "    return cost / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972efb1d",
   "metadata": {},
   "source": [
    "With this, we can check the performance of our randomly initialized network in classifying some of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainData[:128]    # select a batch of images\n",
    "labels = trainLabels[:128] # and corresponding labels\n",
    "\n",
    "cost_function(params,batch,labels,net_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ea64f",
   "metadata": {},
   "source": [
    "Now, what is missing is a function to compute the gradients of the cost function. This is easily solved using `jax.grad` for automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_gradients = jax.grad(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6123238",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ee062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, labels):\n",
    "    '''This is a helper function that counts how many of the given predictions match the labels.\n",
    "    Args:\n",
    "    * `predictions`: Predictions from neural network (=activations on output layer)\n",
    "    * `labels`: correct labels\n",
    "    Returns: Number of correct predictions, i.e., number of cases, in which the index of the maximal \n",
    "    activation matches the given label.\n",
    "    '''\n",
    "    \n",
    "    pred_labels = jnp.argmax(predictions, axis=1)\n",
    "\n",
    "    return jnp.where(pred_labels==labels)[0].shape[0]\n",
    "\n",
    "\n",
    "# Get a key for the PRNG\n",
    "prng_key = jax.random.PRNGKey(123)\n",
    "\n",
    "# Here we define the hyperparamters\n",
    "num_epochs = 10 # Number of epochs to loop over\n",
    "learning_rate = 0.002 # Learning rate\n",
    "batch_size = 128 # Size of mini-batches\n",
    "\n",
    "# Compute the number of mini-batches that matches the chosen mini-batch size\n",
    "batch_number = trainData.shape[0] // batch_size\n",
    "\n",
    "# Evaluate network and assess performance\n",
    "predictions = neural_network(params, testData, net_layers)\n",
    "current_cost = cost_function(params, testData, testLabels, net_layers)\n",
    "correct_predictions = evaluate_predictions(predictions, testLabels)\n",
    "print(\"  Initial cost: %f\" % (current_cost))\n",
    "print(\"  Correctly predicted labels: %d / %d\" % (correct_predictions, len(testLabels)))\n",
    "\n",
    "# Training loop over epochs\n",
    "for n in range(num_epochs):\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    print(\"* Episode %d\" % (n))\n",
    "    \n",
    "    # Generate batches from randomly permuted data\n",
    "    prng_key, tmp_key = jax.random.split(prng_key) # jax-style treatment of random numbers\n",
    "    batches = (jax.random.permutation(tmp_key, trainData)[:batch_number*batch_size].reshape(-1,128,28,28), \n",
    "               jax.random.permutation(tmp_key, trainLabels)[:batch_number*batch_size].reshape(-1,128))\n",
    "    \n",
    "    # Loop over mini-batches\n",
    "    for samples, labels in zip(*batches):\n",
    "\n",
    "        # compute gradients\n",
    "        grads = jax.jit(cost_function_gradients, static_argnums=3)(params, samples, labels, net_layers)\n",
    "        \n",
    "        # Perform SGD parameter update step\n",
    "        params = jax.tree_util.tree_multimap(lambda a,b: a-learning_rate*b, params, grads)\n",
    "\n",
    "    # Evaluate network and assess performance\n",
    "    predictions = neural_network(params, testData, net_layers)\n",
    "    current_cost = cost_function(params, testData, testLabels, net_layers)\n",
    "    correct_predictions = evaluate_predictions(predictions, testLabels)\n",
    "    print(\"  Current cost: %f\" % (current_cost))\n",
    "    print(\"  Correctly predicted labels: %d / %d\" % (correct_predictions, len(testLabels)))\n",
    "    print(\"  -- Time for episode: %fs\" % (time.perf_counter()-tic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_seminar]",
   "language": "python",
   "name": "conda-env-ml_seminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
