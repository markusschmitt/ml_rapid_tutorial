{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dcbfc",
   "metadata": {},
   "source": [
    "# Rapid intro to supervised learning with neural nets I: from scratch\n",
    "\n",
    "This notebook gives a rapid introduction to supervised learning with neural networks. The example is based on [Chapter 1 of Nielsen's online book \"Neural Networks and Deep Learning\"](http://neuralnetworksanddeeplearning.com/chap1.html) and it guides you to set up the neural network training completely from scratch.\n",
    "\n",
    "For further reading I recommend also the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823).\n",
    "\n",
    "We also employ JAX functionality, in particular, [vectorization](https://jax.readthedocs.io/en/latest/jax.html?highlight=vmap#vectorization-vmap) and [random numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html?highlight=random).\n",
    "\n",
    "**Notice:** In order to use JAX function transformations (like vectorization) we have to replace the Numpy library with its JAX version, which has the same interface. For example, instead of `np.array` all our arrays will be `jnp.array`s. The same for functions, e.g. instead of `np.dot` we have to use `jnp.dot` for the dot-product.\n",
    "\n",
    "**This is the TEMPLATE notebook. In the code below you need to fill in missing parts that are marked by comments starting with `# !`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8ac0a",
   "metadata": {},
   "source": [
    "## The MNIST hand-written digits data set\n",
    "\n",
    "Let's first get a simple exemplary data set - the MNIST hand-written digits. The following cell downloads both the test and training parts of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = tfds.as_numpy(tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False))['image'].reshape(-1,28,28)\n",
    "trainLabels = tfds.as_numpy(tfds.load('mnist', split='train', batch_size=-1, shuffle_files=False))['label']\n",
    "\n",
    "testData = tfds.as_numpy(tfds.load('mnist', split='test', batch_size=-1, shuffle_files=False))['image'].reshape(-1,28,28)\n",
    "testLabels = tfds.as_numpy(tfds.load('mnist', split='test', batch_size=-1, shuffle_files=False))['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64bbc",
   "metadata": {},
   "source": [
    "`trainData` is now a Numpy array of shape `(60000,28,28)`, meaning that we have 60k images of 28$\\times$28 pixels (grayscale), each showing one hand-written digit. `trainLabels` holds the corresponding *labels*, i.e. an integer for each image, stating which digit it shows.\n",
    "\n",
    "Let's have a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7857b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(4,4,figsize=(7,7))\n",
    "\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        idx = random.randint(0,60000)\n",
    "        ax.imshow(trainData[idx])\n",
    "        ax.set_title(\"This is a %d\" % (trainLabels[idx]))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca229b9f",
   "metadata": {},
   "source": [
    "Our goal is now to train a neural network, which takes the images as input, and processes them in such a way as to tell us which digit it is. For this purpose, we can take a supervised learning approach, because we have a *labeled data set*, where we know for each example the answer we would like the neural network to give.\n",
    "\n",
    "Formally, we would like to find a function $f$, which maps every training sample $x^{(j)}$ to the corresponding label $y^{(j)}$:\n",
    "\n",
    "$$y^{(j)}=f(x^{(j)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d130d",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Neural networks are constructed as alternating sequences of affine-linear and non-linear maps. Therefore, it is natural to talk about *layers* as building blocks. The output of the $l$-th layer is a vector of *activations* $\\mathbf{a}^{(l)}\\equiv(a_1^{(l)},\\ldots,a_{N_l}^{(l)})$ and $N_l$ is the *width* of the $l$-th layer. This output is obtained by processing the activations of the previous layer by an affine-linear map followed by a non-linear map:\n",
    "\n",
    "$$a_i^{(l)} = \\sigma\\bigg(\\sum_j W_{ij}^{(l)}a_j^{(l-1)}+b_j^{(l)}\\bigg)$$\n",
    "\n",
    "Here the **weights** $W_{ij}^{(l)}$ are the entries of a $N_l\\times N_{l-1}$-matrix and $b_j^{(l)}$ are $N_l$ **biases**. $\\sigma$ is some non-linear function. A common choice is the *sigmoid* function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + jnp.exp(-x))\n",
    "\n",
    "plt.plot(np.arange(-10.,10.),sigmoid(np.arange(-10,10)))\n",
    "plt.ylabel(r\"$x$\")\n",
    "plt.ylabel(r\"$\\sigma(x)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d34cc",
   "metadata": {},
   "source": [
    "A neural network layer is commonly represented graphically as follows:\n",
    "\n",
    "<img src=\"figures/ann_layer.png\" style=\"width:450px;\">\n",
    "\n",
    "A neural network is a function constructed from neural network layers by stacking them on top of each other. The input is interpreted as the activations of the $0$-th layer:\n",
    "\n",
    "$$\\mathbf{a}^{(0)}(\\mathbf{x}) = \\mathbf{x}$$\n",
    "\n",
    "The activations in the following layers, $\\mathbf{a}^{(l)}(\\mathbf{x})$, are obtained accordin to the iterative prescription given above. The resulting activations of the last layer, $\\mathbf{a}^{(D)}$ constitute the output of the neural network. Therefore, a neural network is a function\n",
    "\n",
    "$$f_\\theta:\\mathbf x\\mapsto f_\\theta(\\mathbf x)$$\n",
    "\n",
    "Here $\\theta$ denotes the **parameters** of the network, i.e., the set of all weights and biases.\n",
    "\n",
    "Remarkably, neural networks are **universal function approximators** in the limit of infinite depth, $D\\to\\infty$, or width, $N_l\\to\\infty$. This means that by choosing the right parameters $\\theta$ the function $f_\\theta$ can arbitrarily accurately approximate any function, if the network is only big enough. *Therefore, we can be optimistic to find also a set of parameters $\\theta$ such that the neural network maps our images of hand-written digits to the corresponding digit.*\n",
    "\n",
    "But first, let us set up our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(dimensions, seed=123):\n",
    "    '''This is a helper function to get a set of random initial parameters for a given network size.\n",
    "    The size is given as a list of widths, one width for each layer.\n",
    "    Args:\n",
    "    * `dimensions`: List of layer widths.\n",
    "    * `seed`: PRNG seed.\n",
    "    Returns:\n",
    "    A dictionary holding a list of \"weights\" and a list of \"biases\", where each entry \n",
    "    is the weight matrix/bias vector of the respective layer.\n",
    "    '''\n",
    "\n",
    "    rndKey = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    params={\"weights\": [], \"biases\": []}\n",
    "    \n",
    "    for j in range(len(dimensions)-1):\n",
    "        rndKey, key1 = jax.random.split(rndKey)\n",
    "        weights=0.01*jax.random.normal(rndKey, (dimensions[j+1],dimensions[j]))\n",
    "        biases=jnp.zeros(dimensions[j+1])\n",
    "        \n",
    "        params[\"weights\"].append(weights)\n",
    "        params[\"biases\"].append(biases)\n",
    "        \n",
    "    return params\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None,0)) # This decorator vectorizes the function\n",
    "def neural_network(params, x):\n",
    "    '''This function evaluates the neural network with the given parameters.\n",
    "    Args:\n",
    "    * `params`: Neural network parameters\n",
    "    * `x`: Input image.\n",
    "    Returns: Obtained activations of the last layer.\n",
    "    '''\n",
    "    \n",
    "    a = x.ravel() # flatten input and assign it to the activations of the zeroth layer\n",
    "    \n",
    "    # ! evaluate the network\n",
    "\n",
    "    # ! return activations of last layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc36704",
   "metadata": {},
   "source": [
    "Now we are set to see what the network thinks about our images of digits.\n",
    "\n",
    "Clearly, there are some constraints on the network size for this purpose. The width of the first layer has to match the number of pixels in the images ($28\\times28$). Also, we would like to have ten numbers as output - the network is supposed to indicate its answer ($0-9$) through the maximum value of the ten outputs. In addition we introduce one intermediate layer of width $N_1=100$.\n",
    "\n",
    "*Remark: here you might get a warning regarding the absence of GPUs. No reason to be concerned.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9eab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! Get random parameters for the desired network size (28*28,100,10)\n",
    "\n",
    "# ! Evaluate the network on the first three examples in our data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54d9f8",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Now, how can we find parameters $\\theta$ that allow our neural network to do the job? How can our network *learn* to predict the correct labels for the input images? For this purpose we set up a *cost function* that defines the objective and quantifies how well the neural network solves the task.\n",
    "\n",
    "$$\\mathcal L_{\\mathcal T}(\\theta)=\\frac{1}{|\\mathcal T|}\\sum_{(x,y)\\in \\mathcal T}\\big(y-f_\\theta(x)\\big)^2$$\n",
    "\n",
    "Here, $\\mathcal T$ denotes the training data set.\n",
    "\n",
    "Our network gives ten numbers as output ($f_\\theta^r(x)$ for $r=0\\ldots 9$) and we would like to interpret it such that the index of the maximal output indicates the digit shown in the input image. Therefore, we rewrite the cost function as\n",
    "\n",
    "$$\\mathcal L_{\\mathcal T}(\\theta)=\\frac{1}{|\\mathcal T|}\\sum_{(x,y)\\in \\mathcal T}\\sum_{r=0}^9\\big(\\delta_{r,y}-f_\\theta^r(x)\\big)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(predictions, labels):\n",
    "    '''This function evaluates the cost function for given predictions and labels\n",
    "    Args:\n",
    "    * `predictions`: Predictions from neural net. Array of shape $|\\mathcal T|$ x 10.\n",
    "    * `labels`: Correct labels for the corresponding images. Array of $|\\mathcal T|$ integers.\n",
    "    Returns: Cost associated with the neural network predictions for the given data.\n",
    "    '''\n",
    "\n",
    "    labels = jax.nn.one_hot(labels, 10) # get one-hot encoding of labels\n",
    "    cost = jnp.sum((predictions-labels)**2)\n",
    "\n",
    "    return cost / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972efb1d",
   "metadata": {},
   "source": [
    "With this, we can check the performance of our randomly initialized network in classifying some of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainData[:128]    # select a batch of images\n",
    "labels = trainLabels[:128] # and corresponding labels\n",
    "\n",
    "# ! compute neural network predictions\n",
    "\n",
    "# ! evaluate the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fe6ec",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent and backpropagation\n",
    "\n",
    "You saw above that our network does not do particularly well yet in classifying the digits. We now have to **train** it and the basic idea is to do **gradient-based optimization** to minimize the cost function.\n",
    "\n",
    "Generally, we can attempt to minimize the cost function using gradient descent, an iterative procedure where in each step the parameter update\n",
    "$$\\theta^{(j+1)}\\leftarrow\\theta^{(j)}-\\eta\\nabla_\\theta\\mathcal L_{\\mathcal T}(\\theta^{(j)})$$\n",
    "is performed with some **learning rate** $\\eta$. This way, we can reduce the loss until we reach a stationary state with $\\nabla_\\theta\\mathcal L(\\theta)=0$. Unfortunately, however, the cost landscape $\\mathcal L_{\\mathcal T}(\\theta)$ is highly non-convex, meaning that it typically comprises an abundant number of local minima and saddle points. Plain gradient descent is prone to getting stuck in very these sub-optimal stationary points.\n",
    "\n",
    "This is one reason why neural networks are in practice trained using **stochastic gradient descent** (SGD) or some more sophisticated variants of it. The term *stochastic* refers to the fact that in each step gradients of the cost function are computed only on a small randomly chosen subset of the full training set $\\mathcal B_j\\subset\\mathcal T$. These subsets $\\mathcal B_j$ are called **(mini-)batches** and the SGD update rule for step number $j$ is\n",
    "$$\\theta^{(j+1)}\\leftarrow\\theta^{(j)}-\\eta\\nabla_\\theta\\mathcal L_{\\mathcal B_j}(\\theta^{(j)})$$\n",
    "The stochastic noise introduced in this way enables us to avoid getting stuck in saddle points and to overcome *cost barries* such that we ultimately reach better minima. Besides that, the batch-wise evaluation of gradients has practical advantages, because typical data sets of interest in machine learning often exceed the available memory capacities, such that computing gradients on the full data set would be extremely costly.\n",
    "\n",
    "But how do we compute these gradients? In fact, this can be done very efficiently thanks to the layered structure of neural networks, which allows us to use the **backpropagation** algorithm. Backpropagation is essentially based on the observation that knowing the gradients of the cost function with respect to activations in the $l+1$-th layer enables us to very easily compute gradients of the cost function with respect to activations in the $l$-th layer because of the chain rule:\n",
    "\n",
    "$$\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial a_i^{(l)}}=\\sum_j\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial a_j^{(l+1)}}\\frac{\\partial a_j^{(l+1)}}{\\partial a_i^{(l)}}$$\n",
    "\n",
    "To write down the backpropagation rules for our fully connected neural network, we introduce, moreover, the pre-activations\n",
    "\n",
    "$$z_i^{(l)}=\\sum_j W_{ij}^{(l)}a_j^{(l-1)}+b_j^{(l)}$$\n",
    "\n",
    "which are related to the activations via $a_i^{(l)}=\\sigma\\big(z_i^{(l)}\\big)$. Including the pre-activations, we can write derivatives of the cost function w.r.t. our variational parameters $W_{ij}^{(l)}$ and $b_j^{(l)}$ as\n",
    "\n",
    "$$\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial W_{ij}^{(l)}}=\\sum_k\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial \n",
    "z_k^{(l)}}\\frac{\\partial z_k^{(l)}}{\\partial W_{ij}^{(l)}}=\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_i^{(l)}}a_j^{(l-1)}$$\n",
    "and\n",
    "$$\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial b_{i}^{(l)}}=\\sum_k\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_k^{(l)}}\\frac{\\partial z_k^{(l)}}{\\partial b_{i}^{(l)}}=\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_i^{(l)}}$$\n",
    "\n",
    "Therefore, we introduce\n",
    "\n",
    "$$\\Delta_j^{(D)}=\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_j^{(D)}}\\equiv\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial a_j^{(D)}}\\sigma'\\big(z_j^{(D)}\\big)\\\\\n",
    "\\Delta_j^{(l)}=\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_j^{(l)}}=\n",
    "\\sum_k\\frac{\\partial\\mathcal L_{\\mathcal B}}{\\partial z_k^{(l+1)}}\\frac{\\partial z_k^{(l+1)}}{\\partial z_j^{(l)}}=\\sum_k\n",
    "\\Delta_k^{(l+1)}W_{kj}^{(l+1)}\\sigma'(z_j^{(l)})\n",
    "$$\n",
    "\n",
    "Here $\\sigma'(\\cdot)$ denotes the derivative of the non-linearity $\\sigma(\\cdot)$. The last four equations give us the prescription to obtain the gradients of the cost function as follows:\n",
    "\n",
    "1. Perform a *forward evaluation* of the network and keep the activations $a_j^{(l)}$ as well as the pre-activations $z_j^{(l)}$ in memory.\n",
    "2. Perform a *backward pass* to iteratively obtain the $\\Delta_j^{(l)}$, and at each step compute the gradients with respect to the variational parameters in the corresponding layer.\n",
    "\n",
    "Notice that due to the linearity of the gradient we can first compute the per-sample gradients $\\nabla_\\theta\\mathcal L_{\\{x\\}}(\\theta)$ for all $x\\in\\mathcal B$ and then obtain the mini-batch gradient as\n",
    "\n",
    "$$\\nabla_\\theta\\mathcal L_{\\mathcal B}(\\theta)=\\sum_{x\\in\\mathcal B}\\nabla_\\theta\\mathcal L_{\\{x\\}}(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    ''' This function evaluates the derivative of the sigmoid non-linearity.\n",
    "    Args:\n",
    "    * `x`: Float.\n",
    "    Returns: $\\sigma'(x)$\n",
    "    '''\n",
    "    # ! compute and return the derivative of the sigmoid function\n",
    "\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None,0,0)) # This decorator vectorizes the function\n",
    "def cost_function_individual_gradients(params, x, label):\n",
    "    '''This function defines the per-sample gradient computed by backpropagation.\n",
    "    Args:\n",
    "    * `params`: Neural network parameters\n",
    "    * `x`: Input image (2D array).\n",
    "    * `label`: Label associated with the input image.\n",
    "    Returns:\n",
    "    Gradient of the cost function for the given sample.\n",
    "    '''\n",
    "    \n",
    "    label = jax.nn.one_hot(label, 10) # get one-hot encoding of label\n",
    "    \n",
    "    a = x.ravel() # flatten input\n",
    "    # Set up storage space for (pre-)activations\n",
    "    a_list = [a]\n",
    "    z_list = []\n",
    "    \n",
    "    # FORWARD PASS\n",
    "    \n",
    "    # ! forward evaluation of the network, storing (pre-)activations\n",
    "\n",
    "\n",
    "    # BACKWARD PASS\n",
    "    # Set up storage space for gradients (layer-wise)\n",
    "    weight_gradients = [jnp.zeros_like(w) for w in params[\"weights\"]]\n",
    "    bias_gradients = [jnp.zeros_like(b) for b in params[\"biases\"]]\n",
    "    \n",
    "    # ! Apply backpropagation rules for top layer\n",
    "    \n",
    "    \n",
    "    for l in range(2,len(a_list)):\n",
    "        \n",
    "        # ! Apply backpropagation rules\n",
    "\n",
    "        \n",
    "    return {\"weights\": weight_gradients, \"biases\": bias_gradients}\n",
    "\n",
    "\n",
    "def cost_function_gradients(params, samples, labels):\n",
    "    '''This function computes the gradients for a given mini-batch of data.\n",
    "    Args:\n",
    "    * `params`: Neural network parameters.\n",
    "    * `samples`: Batch of training images.\n",
    "    * `labels`: Labels corresponding to the given images.\n",
    "    Returns:\n",
    "    Gradients of the cost function evaluated on the mini-batch of training data.\n",
    "    '''\n",
    "    \n",
    "    # Evaluate individual gradients\n",
    "    grads = cost_function_individual_gradients(params, samples, labels)\n",
    "    \n",
    "    # Sum up gradients\n",
    "    for j in range(len(params[\"weights\"])):\n",
    "        grads[\"weights\"][j] = jnp.sum(grads[\"weights\"][j], axis=0)\n",
    "        grads[\"biases\"][j] = jnp.sum(grads[\"biases\"][j], axis=0)\n",
    "    \n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fc6d3",
   "metadata": {},
   "source": [
    "## Training and test data sets\n",
    "\n",
    "You saw that above we loaded `trainData` as well as `testData`. The reason for this is that we would like to test the performance of our trained network on examples that it has not seen before. This check tells us how well the network *learned to generalize* from the examples in our training data. Moreover, we want to avoid that our network learns to solve the task by exploiting specific features that are only present in our training data set (*overfitting*). Once the network starts to overfit, the cost achieved on the test data set (**test error**) grows as training progresses, i.e. the generalization quality deteriorates. Therefore, it is important to monitor the test error during training.\n",
    "\n",
    "## Training loop\n",
    "\n",
    "We are not ready to compose the training loop from all the pieces defined above. Each iteration of the loop is called an **epsiode**. In each episode, the training data set is shuffled and split into equally sized **(mini-)batches**. Then the gradients are computed sequentially for each mini-batch and the parameters of the neural network are updated according to the SGD update rule.\n",
    "\n",
    "After completion of each episode we assess the performance of the network by evaluating the cost function on the test data set and, in addition, by counting how many of the images in the test data set are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ee062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions, labels):\n",
    "    '''This is a helper function that counts how many of the given predictions match the labels.\n",
    "    Args:\n",
    "    * `predictions`: Predictions from neural network (=activations on output layer)\n",
    "    * `labels`: correct labels\n",
    "    Returns: Number of correct predictions, i.e., number of cases, in which the index of the maximal \n",
    "    activation matches the given label.\n",
    "    '''\n",
    "    \n",
    "    pred_labels = jnp.argmax(predictions, axis=1)\n",
    "\n",
    "    return jnp.where(pred_labels==labels)[0].shape[0]\n",
    "\n",
    "\n",
    "# Get a key for the PRNG\n",
    "prng_key = jax.random.PRNGKey(123)\n",
    "\n",
    "# Here we define the hyperparamters\n",
    "num_epochs = 10 # Number of epochs to loop over\n",
    "learning_rate = 0.001 # Learning rate\n",
    "batch_size = 128 # Size of mini-batches\n",
    "\n",
    "# Compute the number of mini-batches that matches the chosen mini-batch size\n",
    "batch_number = trainData.shape[0] // batch_size\n",
    "\n",
    "# Evaluate network and assess performance\n",
    "predictions = neural_network(params, testData)\n",
    "current_cost = cost_function(neural_network(params, testData), testLabels)\n",
    "correct_predictions = evaluate_predictions(predictions, testLabels)\n",
    "print(\"Initial cost: %f\" % (current_cost))\n",
    "print(\"Correctly predicted labels: %d / %d\" % (correct_predictions, len(testLabels)))\n",
    "\n",
    "# Training loop over epochs\n",
    "for n in range(num_epochs):\n",
    "    \n",
    "    print(\"Episode %d\" % (n))\n",
    "    \n",
    "    # Generate batches from randomly permuted data\n",
    "    prng_key, tmp_key = jax.random.split(prng_key) # jax-style treatment of random numbers\n",
    "    batches = (jax.random.permutation(tmp_key, trainData)[:batch_number*batch_size].reshape(-1,128,28,28), \n",
    "               jax.random.permutation(tmp_key, trainLabels)[:batch_number*batch_size].reshape(-1,128))\n",
    "    \n",
    "    # Loop over mini-batches\n",
    "    for samples, labels in zip(*batches):\n",
    "\n",
    "        # compute gradients\n",
    "        grads = cost_function_gradients(params, samples, labels)\n",
    "        \n",
    "        # Perform SGD parameter update step\n",
    "        for j in range(len(params[\"weights\"])):\n",
    "            params[\"weights\"][j] -= learning_rate * grads[\"weights\"][j]\n",
    "            params[\"biases\"][j] -= learning_rate * grads[\"biases\"][j]\n",
    "\n",
    "    # Evaluate network and assess performance\n",
    "    predictions = neural_network(params, testData)\n",
    "    current_cost = cost_function(predictions, testLabels)\n",
    "    correct_predictions = evaluate_predictions(predictions, testLabels)\n",
    "    print(\"Current cost: %f\" % (current_cost))\n",
    "    print(\"Correctly predicted labels: %d / %d\" % (correct_predictions, len(testLabels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_seminar]",
   "language": "python",
   "name": "conda-env-ml_seminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
