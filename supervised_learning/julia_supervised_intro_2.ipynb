{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using PyPlot\n",
    "using Random, Statistics\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid intro to supervised learning with neural nets I: from scratch\n",
    "\n",
    "This notebook gives a rapid introduction to supervised learning with neural networks. The example is based on [Chapter 1 of Nielsen's online book \"Neural Networks and Deep Learning\"](http://neuralnetworksanddeeplearning.com/chap1.html) and it guides you to set up the neural network training using the [Flux](https://fluxml.ai/Flux.jl/stable/) Julia package.\n",
    "\n",
    "For further reading I recommend also the review article [\"A high-bias, low-variance introduction to Machine Learning for physicists\"](https://arxiv.org/abs/1803.08823)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few words about Flux\n",
    "\n",
    "[Flux](https://fluxml.ai/Flux.jl/stable/) provides functionality for deep learning that is similar to the established ML libraries [TensorFlow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org). This means, in particular, automatic differentiation and simple ways to construct deep learning models (=neural networks).\n",
    "\n",
    "### Automatic differentiation\n",
    "With [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) we can let the computer compute gradients of arbitrary functions. In Flux the function `gradient()` takes a function and an input for the fuction as argument and the gradient of the given function. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(x)=x^2\n",
    "\n",
    "f_prime(x)=Tracker.data(Flux.gradient(f,x)[1])\n",
    "\n",
    "x=-1.0:0.1:1.0\n",
    "plot(x,f.(x),label=L\"$f(x)$\")\n",
    "plot(x,f_prime.(x),label=L\"$f'(x)$\")\n",
    "xlabel(L\"$x$\")\n",
    "legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: For the purpose of automatic differentiation Flux introduces *tracked* data types. Therefore, the plain `Flux.gradient(f,x)[1]` returns a tracked `Float` and we have to call the `Tracker.data` function in order to convert it into a plain `Float`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST hand-written digits data set\n",
    "\n",
    "Let's first get a simple exemplary data set - the MNIST hand-written digits. The following cell downloads both the test and training parts of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full training set\n",
    "train_x, train_y = float.(MNIST.traindata())\n",
    "# load full test set\n",
    "test_x,  test_y  = float.(MNIST.testdata());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trainData` is now a array of shape `(28,28, 60000)`, meaning that we have 60k images of 28$\\times$28 pixels (grayscale), each showing one hand-written digit. `trainLabels` holds the corresponding *labels*, i.e. an integer for each image, stating which digit it shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a neural network model using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_model(layers; imgsize=(28,28))\n",
    "    m = Dense(prod(imgsize), layers[1], sigmoid)\n",
    "    for j in 2:length(layers)\n",
    "        m = Chain(m, Dense(layers[j-1], layers[j],sigmoid))\n",
    "    end\n",
    "    return m\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again what the network thinks about our images of digits. For this purpose we define `initialize_network` and `neural_network` analogous to part I of the tutorial, but this time based on our `MyNet` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_layers=[100,10]\n",
    "\n",
    "neural_network = build_model(net_layers)\n",
    "params = Flux.params(neural_network) \n",
    "\n",
    "neural_network(reshape(train_x[:,:,1],28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a cost function. This is the same as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function(predictions, labels)\n",
    "    \"\"\"This function evaluates the cost function for given predictions and labels\n",
    "    Args:\n",
    "    * predictions: Predictions from neural net. Array of shape mathcal T x 10.\n",
    "    * labels: Correct labels for the corresponding images. Array of mathcal T integers.\n",
    "    Returns: Cost associated with the neural network predictions for the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = Flux.onehotbatch(labels, 0:9)\n",
    "\n",
    "    cost = sum((predictions-labels).^2)\n",
    "    return cost / size(labels)[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can check the performance of our randomly initialized network in classifying some of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_x[:,:,1:128]    # select a batch of images\n",
    "labels = train_y[1:128] # and corresponding labels\n",
    "\n",
    "# ! compute neural network predictions\n",
    "predictions = neural_network(reshape(batch, 28*28,size(batch)[3]))\n",
    "\n",
    "# ! evaluate the cost function\n",
    "cost_function(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is missing is a function to compute the gradients of the cost function. This is easily solved using `Flux.gradient()` for automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function_gradient(net, params, batch, labels)\n",
    "    return Flux.gradient(() -> cost_function(net(reshape(batch, 28*28,size(batch)[3])),labels), params) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function evaluate_predictions(predictions, labels)\n",
    "    \"\"\"This is a helper function that counts how many of the given predictions match the labels.\n",
    "    Args:\n",
    "    * `predictions`: Predictions from neural network (=activations on output layer)\n",
    "    * `labels`: correct labels\n",
    "    Returns: Number of correct predictions, i.e., number of cases, in which the index of the maximal \n",
    "    activation matches the given label.\n",
    "    \"\"\"\n",
    "    pred_labels = [Int(findmax(predictions[:,i])[2])-1 for i in 1:size(predictions)[2]]\n",
    "        \n",
    "    return sum(pred_labels .== labels)\n",
    "end\n",
    "\n",
    "\n",
    "prng_key = Random.seed!(1234)\n",
    "\n",
    "neural_network = build_model(net_layers)\n",
    "params = Flux.params(neural_network) \n",
    "\n",
    "# Here we define the hyperparamters\n",
    "num_epochs = 10 # Number of epochs to loop over\n",
    "learning_rate = 0.001 # Learning rate\n",
    "batch_size = 128 # Size of mini-batches\n",
    "\n",
    "# Compute the number of mini-batches that matches the chosen mini-batch size\n",
    "batch_number = floor(Int,size(train_x)[end] / batch_size)\n",
    "\n",
    "# Evaluate network and assess performance\n",
    "predictions = neural_network(reshape(test_x,28*28,size(test_x)[3]))\n",
    "current_cost = cost_function(predictions, test_y)\n",
    "correct_predictions = evaluate_predictions(predictions, test_y)\n",
    "println(\"Initial cost: $(current_cost)\")\n",
    "println(\"Correctly predicted labels: $(correct_predictions) / $(length(test_y))\")\n",
    "\n",
    "for n in 1:num_epochs\n",
    "    \n",
    "    println(\"Episode $(n)\")\n",
    "    order = shuffle(1:length(train_y))\n",
    "    samples, labels = ( reshape(train_x[:,:,order][:,:,1:Int(batch_number*batch_size)], 28,28,128,:), \n",
    "        reshape(train_y[order][1:Int(batch_number*batch_size)], 128,:))\n",
    "\n",
    "    \n",
    "    for i in 1:batch_number\n",
    "\n",
    "        # Compute gradients\n",
    "        gs=cost_function_gradient(neural_network, params, samples[:,:,:,i], labels[:,i])\n",
    "  \n",
    "        # Perform SGD parameter update step\n",
    "        for p in params\n",
    "            Flux.Optimise.update!(p,-learning_rate*gs[p])\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    # Evaluate network and assess performance\n",
    "    predictions = neural_network(reshape(test_x,28*28,size(test_x)[3]))\n",
    "    current_cost = cost_function(predictions, test_y)    \n",
    "    correct_predictions = evaluate_predictions(predictions, test_y)\n",
    "    println(\"Current cost: $(current_cost)\")\n",
    "    println(\"Correctly predicted labels: $(correct_predictions)/$(length(test_y))\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
